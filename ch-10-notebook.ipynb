{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a7a89e",
   "metadata": {},
   "source": [
    "# üß† Chapter 10: Introduction to Artificial Neural Networks (ANNs) with Keras\n",
    "\n",
    "This notebook provides a practical, hands-on guide to understanding and building neural networks using Keras. It covers from biological inspiration to implementing models for classification and regression tasks.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b496fb1",
   "metadata": {},
   "source": [
    "## I. From Biological to Artificial Neurons\n",
    "\n",
    "### üß¨ Biological Neurons\n",
    "\n",
    "- Biological neurons receive signals, process them, and transmit signals to other neurons.\n",
    "- They fire when input signals exceed a certain threshold.\n",
    "\n",
    "### ü§ñ Logical Computations with Perceptrons\n",
    "\n",
    "A perceptron is a simple model of a neuron that performs logical operations like AND, OR, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d53b40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND gate outputs: [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Import numpy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "def perceptron(X, weights, bias):\n",
    "    \"\"\"Simple perceptron function for binary output\"\"\"\n",
    "    return (np.dot(X, weights) + bias > 0).astype(int)\n",
    "\n",
    "# Define input data for AND gate\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "weights = np.array([1, 1])  # weights for inputs\n",
    "bias = -1.5  # bias term\n",
    "\n",
    "# Test perceptron for AND gate\n",
    "output = perceptron(X, weights, bias)\n",
    "print(\"AND gate outputs:\", output)  # Should be [0, 0, 0, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c3acc",
   "metadata": {},
   "source": [
    "### üîÅ From Perceptron to Multilayer Perceptron (MLP)\n",
    "\n",
    "- **Perceptron**: Single-layer model; linear decision boundary.\n",
    "- **MLP**: Multiple layers with nonlinear activations; capable of learning complex patterns.\n",
    "- Trained with **backpropagation** and gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cd901",
   "metadata": {},
   "source": [
    "## II. Implementing MLPs with Keras\n",
    "\n",
    "### A. Installing TensorFlow\n",
    "\n",
    "Make sure you have TensorFlow installed:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917551d",
   "metadata": {},
   "source": [
    "### B. Building an Image Classifier (MNIST) using the Sequential API\n",
    "\n",
    "We'll load the MNIST dataset and build a simple neural network to classify handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa7ff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.1422 - accuracy: 0.9564 - val_loss: 0.1072 - val_accuracy: 0.9682\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1043 - accuracy: 0.9682 - val_loss: 0.0968 - val_accuracy: 0.9712\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0862 - accuracy: 0.9732 - val_loss: 0.0897 - val_accuracy: 0.9740\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0750 - accuracy: 0.9770 - val_loss: 0.0892 - val_accuracy: 0.9748\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0657 - accuracy: 0.9803 - val_loss: 0.0854 - val_accuracy: 0.9758\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0,1]\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Build the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),  # Flatten images\n",
    "    keras.layers.Dense(300, activation=\"relu\"),  # First hidden layer\n",
    "    keras.layers.Dense(100, activation=\"relu\"),  # Second hidden layer\n",
    "    keras.layers.Dense(10, activation=\"softmax\")  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c95e2",
   "metadata": {},
   "source": [
    "### C. Building a Regression MLP (Sequential API)\n",
    "\n",
    "Let's create a simple regression model to fit synthetic data y = 3x + 5 + noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477113c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4.5434\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6224\n",
      "... (outputs truncated for brevity) ...\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0199\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 1)\n",
    "y = 3 * X + 5 + 0.1 * np.random.randn(1000, 1)\n",
    "\n",
    "# Define the model\n",
    "reg_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(20, activation=\"relu\", input_shape=[1]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile with MSE loss\n",
    "reg_model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "\n",
    "# Train the model\n",
    "reg_model.fit(X, y, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9107f1",
   "metadata": {},
   "source": [
    "### D. Functional API for Complex Models\n",
    "\n",
    "The Functional API allows building models with multiple inputs, outputs, or complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f576bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Model with multiple layers using Functional API\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define Input layer\n",
    "input_ = keras.layers.Input(shape=[8])\n",
    "# Hidden layers\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "# Output layer\n",
    "output = keras.layers.Dense(1)(hidden2)\n",
    "\n",
    "# Instantiate model\n",
    "model_func = keras.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "# Compile\n",
    "model_func.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25446c",
   "metadata": {},
   "source": [
    "### E. Subclassing API (Dynamic Models)\n",
    "\n",
    "Subclassing allows creating models with custom behavior, flexible for complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06c1cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2347\n",
      "Epoch 2/2\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d20d1b973a0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a custom model by subclassing keras.Model\n",
    "@keras.utils.register_keras_serializable()\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)  # Handle base model arguments like 'trainable'\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"relu\")\n",
    "        self.hidden2 = keras.layers.Dense(30, activation=\"relu\")\n",
    "        self.output_layer = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.hidden1(inputs)\n",
    "        x = self.hidden2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# Instantiate and compile\n",
    "subclassed_model = MyModel()\n",
    "subclassed_model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "# Create appropriate dummy data (regression problem)\n",
    "X_train = np.random.random((1000, 8))  # 8 input features\n",
    "y_train = np.random.random((1000, 1))  # Continuous output for MSE loss\n",
    "\n",
    "# Train\n",
    "subclassed_model.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306225f",
   "metadata": {},
   "source": [
    "### F. Saving and Restoring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe542004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "subclassed_model.save(\"my_mnist_model.keras\")\n",
    "\n",
    "# Load the model later\n",
    "restored_model = keras.models.load_model(\"my_mnist_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed575e",
   "metadata": {},
   "source": [
    "### G. Using Callbacks for Improved Training\n",
    "\n",
    "- **ModelCheckpoint**: saves best model during training\n",
    "- **EarlyStopping**: stops training when validation performance stops improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47fa2249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0948 - val_loss: 0.0730\n",
      "Epoch 2/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0881 - val_loss: 0.0725\n",
      "Epoch 3/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0836 - val_loss: 0.0727\n",
      "Epoch 4/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0841 - val_loss: 0.0722\n",
      "Epoch 5/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0830 - val_loss: 0.0722\n",
      "Epoch 6/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0832 - val_loss: 0.0711\n",
      "Epoch 7/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0772 - val_loss: 0.0730\n",
      "Epoch 8/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0850 - val_loss: 0.0713\n",
      "Epoch 9/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0804 - val_loss: 0.0716\n",
      "Epoch 10/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0796 - val_loss: 0.0724\n",
      "Epoch 11/20\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0853 - val_loss: 0.0719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d20d19b0160>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define callbacks\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train with callbacks\n",
    "restored_model.fit(X_train, y_train, epochs=20,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[checkpoint_cb, early_stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754f3e0",
   "metadata": {},
   "source": [
    "### H. TensorBoard for Visualization\n",
    "\n",
    "- Use TensorBoard to visualize training metrics.\n",
    "- Run the command in terminal after training:\n",
    "  \n",
    "```bash\n",
    "tensorboard --logdir=logs/mnist\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "701c5768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0819 - val_loss: 0.0722\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0843 - val_loss: 0.0719\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0836 - val_loss: 0.0721\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0822 - val_loss: 0.0717\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0789 - val_loss: 0.0719\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0769 - val_loss: 0.0729\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0833 - val_loss: 0.0713\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0810 - val_loss: 0.0726\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0835 - val_loss: 0.0736\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0763 - val_loss: 0.0731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d20d19b2fe0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up logs for TensorBoard\n",
    "import os\n",
    "logdir = os.path.join(\"logs\", \"mnist\")\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Train with TensorBoard callback\n",
    "restored_model.fit(X_train, y_train, epochs=10,\n",
    "          validation_split=0.1, callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099cfe84",
   "metadata": {},
   "source": [
    "## III. Fine-Tuning Neural Network Hyperparameters\n",
    "\n",
    "| Hyperparameter          | Effect                                              |\n",
    "|-------------------------|-----------------------------------------------------|\n",
    "| Number of layers       | More layers = higher capacity                      |\n",
    "| Neurons per layer      | More neurons = more complexity                     |\n",
    "| Learning rate          | Too small = slow learning; too large = divergence |\n",
    "| Batch size             | Smaller = noisier updates; larger = smoother learning |\n",
    "| Regularization, Dropout| Prevent overfitting                                |\n",
    "\n",
    "Experiment with these parameters to optimize your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae4a10",
   "metadata": {},
   "source": [
    "## IV. Exercises to Practice\n",
    "\n",
    "1. Build an MLP for the Boston Housing dataset to predict house prices.\n",
    "2. Explore different learning rates and batch sizes to see their effects.\n",
    "3. Add Dropout layers to your existing models.\n",
    "4. Create models with multiple inputs/outputs using the Functional API.\n",
    "5. Visualize training curves with TensorBoard and fine-tune accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
