{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ace31b3",
   "metadata": {},
   "source": [
    "# 🧩 Chapter 12: Custom Models & Training with TensorFlow — Practical Guide\n",
    "\n",
    "This notebook provides hands-on examples for customizing models and training procedures in TensorFlow. Each section includes explanations and code to run in a Jupyter environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd882a87",
   "metadata": {},
   "source": [
    "---\n",
    "## I. A Quick Tour of TensorFlow\n",
    "\n",
    "TensorFlow primarily uses `tf.Tensor` objects, which are similar to NumPy arrays but are optimized for GPU acceleration, automatic differentiation, and graph optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf16d03",
   "metadata": {},
   "source": [
    "---\n",
    "## II. Using TensorFlow like NumPy\n",
    "\n",
    "Let's explore tensors and operations with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5861e475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:39:54.106571: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-18 08:39:54.147571: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-18 08:39:54.200133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750225194.251928    7352 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750225194.267279    7352 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750225194.344058    7352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750225194.344112    7352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750225194.344130    7352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750225194.344133    7352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-18 08:39:54.364066: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum:\n",
      " tf.Tensor(\n",
      "[[ 6.  8.]\n",
      " [10. 12.]], shape=(2, 2), dtype=float32)\n",
      "MatMul:\n",
      " tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 08:40:02.055674: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define two constant tensors\n",
    "a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "\n",
    "# Basic tensor operations\n",
    "print(\"Sum:\\n\", a + b)\n",
    "print(\"MatMul:\\n\", tf.matmul(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a3c01",
   "metadata": {},
   "source": [
    "### Tensors and NumPy conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a0ad6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor to NumPy and back: tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Convert tensor to NumPy, then back to tensor\n",
    "c_np = a.numpy()\n",
    "d = tf.constant(c_np)\n",
    "print(\"Tensor to NumPy and back:\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bf455",
   "metadata": {},
   "source": [
    "### Type conversions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df55c868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casted tensor:\n",
      " tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Cast tensor to integer type\n",
    "e = tf.cast(a, tf.int32)\n",
    "print(\"Casted tensor:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c0401",
   "metadata": {},
   "source": [
    "### Variables (mutable tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37fa7fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated variable v:\n",
      " <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[2., 3.],\n",
      "       [4., 5.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# Define a variable tensor\n",
    "v = tf.Variable([[1.0, 2.0], [3.0, 4.0]])\n",
    "# Update the variable\n",
    "v.assign_add([[1.0, 1.0], [1.0, 1.0]])\n",
    "print(\"Updated variable v:\\n\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db04153",
   "metadata": {},
   "source": [
    "## III. Customizing Models & Training Algorithms\n",
    "\n",
    "Let's explore how to define custom loss functions, layers, models, and training procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8edfd",
   "metadata": {},
   "source": [
    "### A. Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aab5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Huber loss function\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    is_small = tf.abs(error) <= delta\n",
    "    small_loss = 0.5 * tf.square(error)\n",
    "    large_loss = delta * (tf.abs(error) - 0.5 * delta)\n",
    "    return tf.where(is_small, small_loss, large_loss)\n",
    "\n",
    "# Example: compiling a model with custom loss\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1)])\n",
    "model.compile(loss=huber_loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593cca0",
   "metadata": {},
   "source": [
    "### B. Saving & Loading Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "675a917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with custom loss\n",
    "model.save(\"my_model.keras\")\n",
    "\n",
    "# Load the model, providing the custom loss function\n",
    "loaded_model = tf.keras.models.load_model(\"my_model.keras\", custom_objects={\"huber_loss\": huber_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160feae0",
   "metadata": {},
   "source": [
    "### C. Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a037618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom layer output shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Example of a custom dense layer\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"zeros\", trainable=True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "# Instantiate and test the custom layer\n",
    "layer = MyDense(10)\n",
    "output = layer(tf.zeros((1, 5)))\n",
    "print(\"Custom layer output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d43067",
   "metadata": {},
   "source": [
    "### D. Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "152d4b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.2859 \n",
      "Epoch 2/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2700\n",
      "Epoch 3/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x723406de0460>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a custom model composed of custom layers\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = MyDense(16)\n",
    "        self.dense2 = MyDense(1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = tf.nn.relu(self.dense1(x))\n",
    "        return self.dense2(x)\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = MyModel()\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Generate dummy data and train\n",
    "X_train = tf.random.uniform((100, 4))\n",
    "y_train = tf.random.uniform((100, 1))\n",
    "model.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6091a2be",
   "metadata": {},
   "source": [
    "### E. Computing Gradients with Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d257d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: 6.0\n",
      "dz/dy: 4.0\n"
     ]
    }
   ],
   "source": [
    "# Example of computing gradients\n",
    "x = tf.Variable(3.0)\n",
    "y = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = x**2 + y**2\n",
    "dz_dx, dz_dy = tape.gradient(z, [x, y])\n",
    "print(\"dz/dx:\", dz_dx.numpy())\n",
    "print(\"dz/dy:\", dz_dy.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0479f0",
   "metadata": {},
   "source": [
    "### F. Custom Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "355689ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3681\n",
      "Epoch 2, Loss: 1.2124\n",
      "Epoch 3, Loss: 1.1268\n",
      "Epoch 4, Loss: 1.0730\n",
      "Epoch 5, Loss: 1.0362\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(4,)),  # ✅ Explicit Input layer\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Define the loss function instance\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define a simple train step\n",
    "@tf.function\n",
    "def train_step(x, y_true):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = loss_fn(y_true, y_pred)  # Correct way to call loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Generate dummy data\n",
    "X = tf.random.normal((100, 4))\n",
    "y = tf.random.normal((100, 1))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    loss = train_step(X, y)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fbde2",
   "metadata": {},
   "source": [
    "---\n",
    "## IV. TensorFlow Functions and Graphs\n",
    "\n",
    "Using `@tf.function` to compile Python functions into high-performance graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8d0f801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2, 3): tf.Tensor(6, shape=(), dtype=int32)\n",
      "f(5, 1): tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Example of a compiled function with @tf.function\n",
    "@tf.function\n",
    "def f(x, y):\n",
    "    if x < y:\n",
    "        return x * y\n",
    "    else:\n",
    "        return x + y\n",
    "\n",
    "print(\"f(2, 3):\", f(2, 3))\n",
    "print(\"f(5, 1):\", f(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3236e80",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Feature | Use Case |\n",
    "| --- | --- |\n",
    "| Tensors & Variables | Core data types, high performance, mutability |\n",
    "| Custom Layers & Models | Reusable, modular components |\n",
    "| Autodiff | Automatic gradient calculation |\n",
    "| Custom Training Loops | Full control over training process |\n",
    "| @tf.function | Accelerated execution via graphs |\n",
    "| Custom Losses & Metrics | Tailored objectives and evaluation |\n",
    "\n",
    "Feel free to experiment with these techniques to build and train sophisticated models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187982e1",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises to Try\n",
    "\n",
    "1. Implement **Poisson or Huber loss** and visualize with sample data.\n",
    "2. Create a **parameterized custom layer** with regularization or constraints.\n",
    "3. Build a custom metric like cosine similarity and include in `model.compile`.\n",
    "4. Write a **training loop** for MNIST instead of `model.fit`.\n",
    "5. Wrap functions with `@tf.function` and compare Eager vs Graph execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
