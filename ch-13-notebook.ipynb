{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8725a845",
   "metadata": {},
   "source": [
    "# 📦 Chapter 13: Loading & Preprocessing Data with TensorFlow — Practical Guide\n",
    "\n",
    "Efficient data handling is crucial for scalable deep learning. This notebook covers the TensorFlow Data API (`tf.data`), TFRecord format, preprocessing techniques, and helpful tools like TF Transform and TensorFlow Datasets (TFDS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f80c9",
   "metadata": {},
   "source": [
    "## I. The Data API (`tf.data.Dataset)`\n",
    "\n",
    "Let's explore how to create and manipulate data pipelines with `tf.data` for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabf44ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 13:06:39.986290: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-18 13:06:40.281777: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-18 13:06:40.504811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750241200.718291    1315 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750241200.779367    1315 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750241201.258033    1315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750241201.258151    1315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750241201.258155    1315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750241201.258158    1315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-18 13:06:41.308076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-18 13:06:49.689005: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Generate dummy image data (1000 images of 28x28 with 3 channels)\n",
    "X = tf.random.uniform((1000, 28, 28, 3))\n",
    "\n",
    "# Generate dummy labels (integers from 0 to 9)\n",
    "y = tf.random.uniform((1000,), maxval=10, dtype=tf.int32)\n",
    "\n",
    "# Create a tf.data.Dataset from tensors\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d243a74",
   "metadata": {},
   "source": [
    "### B. Chain transformations: shuffle, batch, data augmentation, prefetching\n",
    "\n",
    "This pipeline shuffles the data, creates batches, applies a random flip (augmentation), and prefetches for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbdeb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(buffer_size=1000)\n",
    "    .batch(32)\n",
    "    .map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d826d",
   "metadata": {},
   "source": [
    "### C. Using the dataset with Keras model training\n",
    "\n",
    "The dataset can be directly fed into `model.fit()` for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62cf4505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0879 - loss: 2.6782 - val_accuracy: 0.1187 - val_loss: 2.3856\n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1389 - loss: 2.3991 - val_accuracy: 0.1000 - val_loss: 2.3436\n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1259 - loss: 2.3233 - val_accuracy: 0.1344 - val_loss: 2.2946\n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0948 - loss: 2.2974 - val_accuracy: 0.0875 - val_loss: 2.3095\n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0942 - loss: 2.3143 - val_accuracy: 0.0969 - val_loss: 2.2975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x79dd3938d660>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28, 3)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the dataset\n",
    "model.fit(dataset, epochs=5, validation_data=dataset.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664549f1",
   "metadata": {},
   "source": [
    "## II. The TFRecord Format\n",
    "\n",
    "TFRecords are a compact, efficient binary format using Protocol Buffers, ideal for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2cad5",
   "metadata": {},
   "source": [
    "### A. Writing TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4db11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(image, label):\n",
    "    # Normalize and cast image to uint8\n",
    "    image_uint8 = tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True)\n",
    "    image_encoded = tf.io.encode_png(image_uint8)\n",
    "    \n",
    "    feature = {\n",
    "        'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_encoded.numpy()])),\n",
    "        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(label)]))\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "# Write TFRecord file\n",
    "with tf.io.TFRecordWriter('data.tfrecord') as writer:\n",
    "    for img, lbl in zip(X.numpy(), y.numpy()):\n",
    "        writer.write(serialize_example(img, lbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7f2c91",
   "metadata": {},
   "source": [
    "### B. Reading TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb2c4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from TFRecord file\n",
    "raw_ds = tf.data.TFRecordDataset(['data.tfrecord'])\n",
    "\n",
    "# Define feature description for parsing\n",
    "feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "def parse_fn(serialized):\n",
    "    parsed = tf.io.parse_single_example(serialized, feature_description)\n",
    "    image = tf.io.decode_png(parsed['image'])\n",
    "    label = parsed['label']\n",
    "    # Normalize image to [0,1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "parsed_ds = raw_ds.map(parse_fn).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806c02f9",
   "metadata": {},
   "source": [
    "### C. SequenceExample for Variable-Length Sequences\n",
    "\n",
    "Use `SequenceExample` when dealing with sequences of variable length, such as time-series or text data. (This is an advanced topic and can be explored further.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ffb37",
   "metadata": {},
   "source": [
    "## III. Preprocessing the Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b7d91",
   "metadata": {},
   "source": [
    "### A. One-Hot Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a88b5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded vectors:\n",
      " [[0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "# Example categorical data\n",
    "categories = tf.constant(['red', 'green', 'blue', 'green', 'red'])\n",
    "\n",
    "# Create a StringLookup layer for one-hot encoding\n",
    "lookup = StringLookup(output_mode='one_hot')\n",
    "lookup.adapt(categories)\n",
    "\n",
    "# Encode new data\n",
    "index = lookup(tf.constant(['red', 'blue', 'green']))\n",
    "print(\"One-hot encoded vectors:\\n\", index.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a1302",
   "metadata": {},
   "source": [
    "### B. Embeddings for Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "038a5c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded shape: (1, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Suppose genre IDs range from 0 to 9\n",
    "embedding = Embedding(input_dim=10, output_dim=4)\n",
    "\n",
    "# Example input IDs\n",
    "input_ids = tf.constant([[1, 3, 7]])\n",
    "embedded = embedding(input_ids)\n",
    "print(\"Embedded shape:\", embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96428cb8",
   "metadata": {},
   "source": [
    "### C. Keras Preprocessing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cbcab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized values:\n",
      " [[-0.63422656  0.7468681 ]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Normalization\n",
    "\n",
    "# Create a normalization layer\n",
    "normalizer = Normalization()\n",
    "\n",
    "# Adapt to some data\n",
    "normalizer.adapt(tf.random.uniform((100, 1)))\n",
    "\n",
    "# Normalize new data\n",
    "normalized_x = normalizer(tf.constant([0.3, 0.7]))\n",
    "print(\"Normalized values:\\n\", normalized_x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c278b05",
   "metadata": {},
   "source": [
    "## IV. TF Transform (TFT)\n",
    "\n",
    "TF Transform allows for consistent preprocessing during training and serving, enabling complex feature engineering pipelines that are portable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4de48",
   "metadata": {},
   "source": [
    "## V. TensorFlow Datasets (TFDS)\n",
    "\n",
    "TFDS provides preloaded, ready-to-use datasets with clean splits and rich metadata, making experimentation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286f036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder /home/ubuntu22/tensorflow_datasets/mnist/3.0.1 has no dataset_info.json\n",
      "/home/ubuntu22/projects/ml-tutorials/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/ubuntu22/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                                                            | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                                                            | 0/2 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                                                            | 0/3 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                                                            | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                                                            | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                                                            | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|                                                                                 | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|█████████████████                                                   | 1/4 [00:00<00:02,  1.11 url/s]\n",
      "Dl Size...:   0%|                                                                                 | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|█████████████████                                                   | 1/4 [00:00<00:02,  1.03 url/s]\n",
      "Dl Size...:   0%|                                                                                 | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|█████████████████                                                   | 1/4 [00:00<00:02,  1.01 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|                                                                                 | 0/9 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|█████████████████                                                   | 1/4 [00:01<00:05,  1.72s/ url]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|                                                                                 | 0/9 [00:01<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|██████████████████████████████████                                  | 2/4 [00:01<00:01,  1.06 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|                                                                                 | 0/9 [00:01<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|██████████████████████████████████                                  | 2/4 [00:01<00:01,  1.06 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|                                                                                 | 0/9 [00:01<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|██████████████████████████████████                                  | 2/4 [00:01<00:01,  1.06 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|                                                                                | 0/10 [00:01<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:  50%|█████████████████████████████▌                             | 1/2 [00:01<00:01,  1.97s/ file]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|██████████████████████████████████                                  | 2/4 [00:02<00:01,  1.06 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|                                                                                | 0/10 [00:02<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|███████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.01 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  50%|██████████████████████████████████                                  | 2/4 [00:03<00:01,  1.06 url/s]\u001b[A\n",
      "Dl Size...:  10%|███████▏                                                                | 1/10 [00:03<00:28,  3.19s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|███████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.01 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  50%|██████████████████████████████████                                  | 2/4 [00:05<00:01,  1.06 url/s]\u001b[A\n",
      "Dl Size...:  20%|██████████████▍                                                         | 2/10 [00:05<00:20,  2.51s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|██████████████████████████████████                                  | 2/4 [00:06<00:01,  1.06 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  30%|█████████████████████▌                                                  | 3/10 [00:06<00:17,  2.51s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████████████████████████████████████                 | 3/4 [00:06<00:00,  1.06 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  30%|█████████████████████▌                                                  | 3/10 [00:06<00:17,  2.51s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████████████████████████████████████                 | 3/4 [00:07<00:00,  1.06 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  30%|█████████████████████▌                                                  | 3/10 [00:07<00:17,  2.51s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████████████████████████████████████████████████                 | 3/4 [00:07<00:00,  1.06 url/s]\u001b[A\u001b[A\n",
      "Dl Size...:  30%|█████████████████████▌                                                  | 3/10 [00:07<00:17,  2.51s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|███████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  1.01 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████████████████████████████████████████████████                 | 3/4 [00:07<00:00,  1.06 url/s]\u001b[A\n",
      "Dl Size...:  40%|████████████████████████████▊                                           | 4/10 [00:07<00:10,  1.76s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|███████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  1.01 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████████████████████████████████████████████████                 | 3/4 [00:09<00:00,  1.06 url/s]\u001b[A\n",
      "Dl Size...:  50%|████████████████████████████████████                                    | 5/10 [00:09<00:08,  1.68s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|███████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  1.01 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████████████████████████████████████████████████                 | 3/4 [00:10<00:00,  1.06 url/s]\u001b[A\n",
      "Dl Size...:  60%|███████████████████████████████████████████▏                            | 6/10 [00:10<00:06,  1.56s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|███████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  1.01 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████████████████████████████████████████████████                 | 3/4 [00:12<00:00,  1.06 url/s]\u001b[A\n",
      "Dl Size...:  70%|██████████████████████████████████████████████████▍                     | 7/10 [00:12<00:04,  1.53s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|███████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  1.01 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████████████████████████████████████████████████                 | 3/4 [00:13<00:00,  1.06 url/s]\u001b[A\n",
      "Dl Size...:  80%|█████████████████████████████████████████████████████████▌              | 8/10 [00:13<00:02,  1.47s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|███████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  1.01 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████████████████████████████████████████████████                 | 3/4 [00:15<00:00,  1.06 url/s]\u001b[A\n",
      "Dl Size...:  90%|████████████████████████████████████████████████████████████████▊       | 9/10 [00:15<00:01,  1.70s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|███████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  1.01 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████████████████████████████████████████████████                 | 3/4 [00:17<00:00,  1.06 url/s]\u001b[A\n",
      "Dl Size...: 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:17<00:00,  1.85s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:18<00:00,  5.28s/ url]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:18<00:00,  1.85s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:18<00:00,  5.28s/ url]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:19<00:00,  1.85s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...:  75%|████████████████████████████████████████████▎              | 3/4 [00:19<00:00,  1.01 file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:19<00:00,  5.28s/ url]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:19<00:00,  1.85s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|███████████████████████████████████████████████████████████| 4/4 [00:19<00:00,  4.77s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:19<00:00,  1.91s/ MiB]\n",
      "Dl Completed...: 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:19<00:00,  4.77s/ url]\n",
      "Generating splits...:   0%|                                                                    | 0/2 [00:00<?, ? splits/s]\n",
      "Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Generating train examples...: 2366 examples [00:01, 2365.37 examples/s]\u001b[A\n",
      "Generating train examples...: 5455 examples [00:02, 2790.95 examples/s]\u001b[A\n",
      "Generating train examples...: 8355 examples [00:03, 2840.51 examples/s]\u001b[A\n",
      "Generating train examples...: 11267 examples [00:04, 2868.45 examples/s]\u001b[A\n",
      "Generating train examples...: 14412 examples [00:05, 2968.08 examples/s]\u001b[A\n",
      "Generating train examples...: 17395 examples [00:06, 2972.92 examples/s]\u001b[A\n",
      "Generating train examples...: 20368 examples [00:07, 2931.80 examples/s]\u001b[A\n",
      "Generating train examples...: 23448 examples [00:08, 2978.14 examples/s]\u001b[A\n",
      "Generating train examples...: 26428 examples [00:09, 2974.05 examples/s]\u001b[A\n",
      "Generating train examples...: 29613 examples [00:10, 3038.48 examples/s]\u001b[A\n",
      "Generating train examples...: 32759 examples [00:11, 3071.08 examples/s]\u001b[A\n",
      "Generating train examples...: 35831 examples [00:15, 1583.95 examples/s]\u001b[A\n",
      "Generating train examples...: 38788 examples [00:16, 1834.73 examples/s]\u001b[A\n",
      "Generating train examples...: 41565 examples [00:17, 2029.71 examples/s]\u001b[A\n",
      "Generating train examples...: 44426 examples [00:18, 2219.24 examples/s]\u001b[A\n",
      "Generating train examples...: 47115 examples [00:19, 2216.02 examples/s]\u001b[A\n",
      "Generating train examples...: 50225 examples [00:20, 2439.40 examples/s]\u001b[A\n",
      "Generating train examples...: 53359 examples [00:21, 2622.10 examples/s]\u001b[A\n",
      "Generating train examples...: 56319 examples [00:22, 2714.12 examples/s]\u001b[A\n",
      "Generating train examples...: 59499 examples [00:23, 2844.72 examples/s]\u001b[A\n",
      "                                                                        \u001b[A\n",
      "Shuffling /home/ubuntu22/tensorflow_datasets/mnist/incomplete.W6FSQI_3.0.1/mnist-train.tfrecord*...:   0%| | 0/60000 [00:0\u001b[A\n",
      "Generating splits...:  50%|██████████████████████████████                              | 1/2 [00:23<00:23, 23.81s/ splits]\u001b[A\n",
      "Generating test examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Generating test examples...: 3013 examples [00:01, 3012.49 examples/s]\u001b[A\n",
      "Generating test examples...: 6219 examples [00:02, 3126.02 examples/s]\u001b[A\n",
      "Generating test examples...: 9346 examples [00:03, 3048.86 examples/s]\u001b[A\n",
      "                                                                      \u001b[A\n",
      "Shuffling /home/ubuntu22/tensorflow_datasets/mnist/incomplete.W6FSQI_3.0.1/mnist-test.tfrecord*...:   0%| | 0/10000 [00:00\u001b[A\n",
      "Generating splits...: 100%|████████████████████████████████████████████████████████████| 2/2 [00:27<00:00, 11.81s/ splits]\u001b[A"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the MNIST dataset\n",
    "ds, info = tfds.load(\"mnist\", split=\"train\", with_info=True)\n",
    "\n",
    "# Preprocess the dataset: normalize images\n",
    "ds = ds.map(lambda x: (tf.cast(x['image'], tf.float32) / 255.0, x['label']))\n",
    "ds = ds.batch(32)\n",
    "\n",
    "# Show dataset info\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af8fec",
   "metadata": {},
   "source": [
    "## Summary of Tools & APIs\n",
    "\n",
    "| Tool/API                 | Use Case                                |\n",
    "| ------------------------ | --------------------------------------- |\n",
    "| **`tf.data.Dataset`**    | Fast, scalable data pipelines           |\n",
    "| **TFRecord**             | Efficient binary storage with ProtoBufs |\n",
    "| **Preprocessing Layers** | In-graph feature transformations        |\n",
    "| **TF Transform**         | Consistent train/serve preprocessing    |\n",
    "| **TFDS**                 | Ready-to-use datasets with metadata     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db8ddff",
   "metadata": {},
   "source": [
    "## 💡 Exercises to Practice\n",
    "\n",
    "1. Write TFRecords from a dataset, then load and decode them.\n",
    "2. Use `tf.data` operations for feature standardization and augmentation.\n",
    "3. Build a pipeline: load TFDS data, preprocess with Keras layers, feed into `model.fit`.\n",
    "4. Explore TF Transform: build a Python preprocessing function, apply in offline & online modes.\n",
    "5. Practice using TFRecord + `SequenceExample` for variable-length inputs (e.g., text sentences or time series)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
