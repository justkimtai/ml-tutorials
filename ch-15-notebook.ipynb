{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40321648",
   "metadata": {},
   "source": [
    "# 🔄 Chapter 15: Sequence Processing with RNNs & CNNs\n",
    "\n",
    "Learn how to work with sequences—like time series or text—using Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7fef4",
   "metadata": {},
   "source": [
    "## I. 🧠 Recurrent Neurons and Layers\n",
    "\n",
    "Recurrent Neural Networks process sequential data by maintaining a hidden state that evolves over time. This allows them to capture temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f752b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 08:10:27.190047: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-19 08:10:27.499377: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-19 08:10:27.728225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750309827.949736    1260 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750309828.028685    1260 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750309828.541350    1260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750309828.541470    1260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750309828.541474    1260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750309828.541477    1260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-19 08:10:28.587578: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-19 08:10:38.710542: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All outputs shape: (2, 10, 5)\n",
      "Final state shape: (2, 5)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define a simple RNN cell with 5 units\n",
    "cell = layers.SimpleRNNCell(units=5)\n",
    "\n",
    "# Wrap the cell into an RNN layer that returns sequences and state\n",
    "rnn_layer = layers.RNN(cell, return_sequences=True, return_state=True)\n",
    "\n",
    "# Generate synthetic data: batch size=2, time steps=10, features=3\n",
    "X = tf.random.uniform((2, 10, 3))\n",
    "\n",
    "# Run data through RNN\n",
    "all_outputs, final_state = rnn_layer(X)\n",
    "\n",
    "# Output shapes\n",
    "print(\"All outputs shape:\", all_outputs.shape)  # (2, 10, 5)\n",
    "print(\"Final state shape:\", final_state.shape)  # (2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0a731",
   "metadata": {},
   "source": [
    "Memory-enhanced cells like **LSTM** and **GRU** help with longer sequences and vanishing gradient problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61671ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final hidden state shape: (2, 5)\n",
      "Final cell state shape: (2, 5)\n"
     ]
    }
   ],
   "source": [
    "# LSTM layer with 5 units\n",
    "lstm_layer = layers.LSTM(5, return_sequences=True, return_state=True)\n",
    "\n",
    "# Process the same input data\n",
    "all_out, final_hidden, final_cell = lstm_layer(X)\n",
    "\n",
    "# Shapes of final states\n",
    "print(\"Final hidden state shape:\", final_hidden.shape)  # (2, 5)\n",
    "print(\"Final cell state shape:\", final_cell.shape)    # (2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf35d5b",
   "metadata": {},
   "source": [
    "## II. 📝 Training RNNs\n",
    "\n",
    "Training RNNs follows the same pattern as other neural networks: compile and fit models with `model.fit()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a847fff3",
   "metadata": {},
   "source": [
    "## III. 📈 Forecasting a Time Series\n",
    "\n",
    "Let's explore a simple time series prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87db73",
   "metadata": {},
   "source": [
    "### A. Baseline Metric\n",
    "\n",
    "First, establish a naive baseline: predict yesterday's value as today's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c4e3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE: 0.006542123263128362\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sine wave time series\n",
    "series = np.sin(np.arange(1000) / 100)\n",
    "\n",
    "# Naive prediction: yesterday's value\n",
    "naive_preds = series[1:]\n",
    "\n",
    "# Compute MAE between actual and naive predictions\n",
    "baseline_mae = np.mean(np.abs(series[:-1] - naive_preds))\n",
    "print(\"Baseline MAE:\", baseline_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb3e26",
   "metadata": {},
   "source": [
    "### B. Simple RNN Model\n",
    "\n",
    "Create a model to learn from the time series data using a simple RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6587f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu22/projects/ml-tutorials/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.0142 - val_loss: 0.0020\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.7160e-04 - val_loss: 2.9905e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.7936e-04 - val_loss: 3.1991e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.4383e-04 - val_loss: 1.9127e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1.0754e-04 - val_loss: 1.2957e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 8.4583e-05 - val_loss: 1.2556e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 6.8464e-05 - val_loss: 9.5178e-05\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.5331e-05 - val_loss: 3.7441e-05\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.1269e-05 - val_loss: 4.9549e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4.1505e-05 - val_loss: 5.5830e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7e0d0cfe3130>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define window size and features\n",
    "n_past = 20\n",
    "n_features = 1\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(series, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - window_size):\n",
    "        X.append(series[i:(i + window_size)])\n",
    "        y.append(series[i + window_size])\n",
    "    return np.expand_dims(np.array(X), axis=-1), np.array(y)\n",
    "\n",
    "# Prepare data\n",
    "X_data, y_data = create_dataset(series, n_past)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    layers.SimpleRNN(20, return_sequences=False, input_shape=(n_past, n_features)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X_data, y_data, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ce186",
   "metadata": {},
   "source": [
    "### C. Deep RNNs\n",
    "\n",
    "Stack multiple RNN layers for more capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e850f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 0.1256 - val_loss: 0.0113\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0038 - val_loss: 9.3827e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0011 - val_loss: 6.7834e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 5.7681e-04 - val_loss: 4.8777e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.7745e-04 - val_loss: 5.3150e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 2.7355e-04 - val_loss: 5.1251e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.3090e-04 - val_loss: 3.2359e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.6483e-04 - val_loss: 2.1019e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.5725e-04 - val_loss: 1.9888e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.1448e-04 - val_loss: 1.5120e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7e0d0c38c7c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a deeper RNN model\n",
    "deep_rnn = Sequential([\n",
    "    layers.SimpleRNN(20, return_sequences=True, input_shape=(n_past, n_features)),\n",
    "    layers.SimpleRNN(20),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "deep_rnn.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "deep_rnn.fit(X_data, y_data, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e5aad",
   "metadata": {},
   "source": [
    "### D. Multi-Step Forecast\n",
    "\n",
    "Use the trained model to predict multiple future steps iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbb6799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Future predictions: [np.float32(-0.53750026), np.float32(-0.5411351), np.float32(-0.54376066), np.float32(-0.5447591), np.float32(-0.54547787), np.float32(-0.544994), np.float32(-0.54423875), np.float32(-0.5422231), np.float32(-0.54106146), np.float32(-0.54056686)]\n"
     ]
    }
   ],
   "source": [
    "# Take the last window from the series\n",
    "last_window = series[-n_past:]\n",
    "\n",
    "# Initialize list to hold predictions\n",
    "preds = []\n",
    "\n",
    "# Prepare input for prediction\n",
    "input_eval = np.expand_dims(last_window, axis=(0,2))  # shape: (1, n_past, 1)\n",
    "\n",
    "# Predict next 10 steps\n",
    "for _ in range(10):\n",
    "    pred = model.predict(input_eval)[0,0]\n",
    "    preds.append(pred)\n",
    "    # Append the predicted value to the input window\n",
    "    input_eval = np.append(input_eval[:,1:,:], [[[pred]]], axis=1)\n",
    "\n",
    "print(\"Future predictions:\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e2aa8",
   "metadata": {},
   "source": [
    "## IV. 🕰️ Handling Long Sequences\n",
    "\n",
    "Sequences can be long, leading to vanishing or exploding gradients. Use strategies like gated cells, gradient clipping, and residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5568320",
   "metadata": {},
   "source": [
    "### A. Vanishing/Exploding Gradients\n",
    "\n",
    "Use gated units such as LSTM/GRU, and techniques like gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c86a3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of gradient clipping with optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174e1b9",
   "metadata": {},
   "source": [
    "### B. Short-Term Memory with CNNs\n",
    "\n",
    "Convolutional models can capture local context and long-range dependencies via dilated convolutions or causal convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99297db9",
   "metadata": {},
   "source": [
    "## ✅ Practical Code Example: Causal 1D Convolution\n",
    "\n",
    "Construct a sequence model combining Conv1D with LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e17a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu22/projects/ml-tutorials/venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.4476 - val_loss: 0.0173\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0181 - val_loss: 0.0049\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0046 - val_loss: 0.0072\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0030 - val_loss: 0.0054\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0025 - val_loss: 0.0048\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0023 - val_loss: 0.0049\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021 - val_loss: 0.0050\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0020 - val_loss: 0.0042\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0018 - val_loss: 0.0040\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017 - val_loss: 0.0035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7e0d0cff8610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "\n",
    "# Build a model with causal convolution followed by LSTM\n",
    "model_causal = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, padding=\"causal\", activation=\"relu\",\n",
    "           input_shape=(n_past, n_features)),\n",
    "    layers.LSTM(20),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model_causal.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model_causal.fit(X_data, y_data, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163dcd93",
   "metadata": {},
   "source": [
    "## 🧠 Chapter Summary\n",
    "\n",
    "- **RNNs** with hidden states handle sequential data effectively.\n",
    "- **Gated units** like LSTM/GRU address training challenges.\n",
    "- **Deep/stacked RNNs** enhance model capacity.\n",
    "- **Multi-step forecasting** involves iterative predictions.\n",
    "- **Convolutional sequence models** capture local and long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdedb8",
   "metadata": {},
   "source": [
    "## 🧪 Exercises to Try\n",
    "\n",
    "1. Train an **LSTM** or **GRU** model and compare with SimpleRNN.\n",
    "2. Use **CNN → LSTM** architecture for improved forecasts.\n",
    "3. Experiment with **gradient clipping**, **residual connections**, or **layer normalization**.\n",
    "4. Build a **Bidirectional RNN** to process sequential data forward and backward.\n",
    "5. Forecast longer sequences (20–50 steps) and compare strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
