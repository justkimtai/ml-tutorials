{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61ed026",
   "metadata": {},
   "source": [
    "# ✍️ Chapter 16: NLP with RNNs & Attention — Practical Guide\n",
    "\n",
    "This notebook provides hands-on, executable code snippets covering:\n",
    "- Character-level RNNs for text generation\n",
    "- Sentiment analysis with LSTM\n",
    "- Encoder-Decoder models for translation\n",
    "- Attention mechanisms and Transformers\n",
    "\n",
    "Feel free to run and modify the code to deepen your understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a0713",
   "metadata": {},
   "source": [
    "## I. Generating Shakespearean Text with a Character RNN\n",
    "\n",
    "We'll train a character-level RNN to generate text in the style of Shakespeare.\n",
    "\n",
    "### A. Create the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99d8fbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 03:26:40.369646: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-20 03:26:40.750052: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-20 03:26:41.026495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750379201.285139    1123 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750379201.359152    1123 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750379201.944354    1123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750379201.944410    1123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750379201.944415    1123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750379201.944420    1123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-20 03:26:42.010019: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Shakespeare text downloaded and saved as 'shakespeare.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 03:26:54.437855: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import requests\n",
    "\n",
    "# Step 1: Download the text from TensorFlow's public URL\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Save it locally as 'shakespeare.txt'\n",
    "with open('shakespeare.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(\"✅ Shakespeare text downloaded and saved as 'shakespeare.txt'\")\n",
    "\n",
    "# Step 3: Load the text\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 4: Create vocabulary\n",
    "vocab = sorted(set(text))\n",
    "char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "idx2char = {i: c for i, c in enumerate(vocab)}\n",
    "\n",
    "# Step 5: Convert entire text to integers\n",
    "text_as_int = tf.constant([char2idx[c] for c in text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd82c04",
   "metadata": {},
   "source": [
    "### B. Split into Sequences & Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e12116",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // seq_length\n",
    "\n",
    "# Create dataset of characters\n",
    "char_ds = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# Batch characters into sequences of length + 1 (for input and target)\n",
    "sequences = char_ds.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# Map to get input-target pairs\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3fc14",
   "metadata": {},
   "source": [
    "### C. Build & Train the Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dec0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,625</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │     \u001b[38;5;34m5,246,976\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)         │        \u001b[38;5;34m66,625\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 16/172\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:39\u001b[0m 3s/step - loss: 4.1211"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "BATCH_SIZE = 64  # Define this first\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(batch_shape=(BATCH_SIZE, None)),  # <-- Define input shape here\n",
    "    layers.Embedding(vocab_size, embedding_dim),\n",
    "    layers.LSTM(rnn_units, return_sequences=True, stateful=True),\n",
    "    layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "# Optional: print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8195930",
   "metadata": {},
   "source": [
    "### D. Generate Fake Shakespearean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c27304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=1000):\n",
    "    # Convert start string to indices\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "\n",
    "    # Reset states at start\n",
    "    model.reset_states()\n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # Sample from the distribution\n",
    "        predicted_id = tf.random.categorical(predictions / 1.0, num_samples=1)[-1,0].numpy()\n",
    "        # Pass the predicted id as the next input\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "print(generate_text(model, start_string=\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5d1c6",
   "metadata": {},
   "source": [
    "### E. Notes on Stateful RNNs\n",
    "\n",
    "- To maintain continuity across batches, set `stateful=True` in the LSTM layer.\n",
    "- When doing so, handle batch resets carefully between epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee66f88",
   "metadata": {},
   "source": [
    "## II. Sentiment Analysis\n",
    "\n",
    "Classify movie reviews as positive or negative using sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000  # Vocabulary size\n",
    "maxlen = 500  # Sequence length\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences to maxlen\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6138c",
   "metadata": {},
   "source": [
    "### A. Build & Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b414463",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Embedding(max_features, 128, input_length=maxlen),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train for a few epochs\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346181fc",
   "metadata": {},
   "source": [
    "### B. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea1f29",
   "metadata": {},
   "source": [
    "### A. Masking in Embedding Layer\n",
    "\n",
    "- Use `mask_zero=True` to ignore padding tokens during training.\n",
    "\n",
    "```python\n",
    "layers.Embedding(max_features, 128, mask_zero=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d390c",
   "metadata": {},
   "source": [
    "### B. Using Pretrained Word Embeddings (e.g., GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55841cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_path = 'glove.6B.100d.txt'  # Ensure this file is available\n",
    "glove_embeddings = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        vector = np.array(parts[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# To use these embeddings, build an embedding matrix aligned with your tokenizer.\n",
    "# For simplicity, code to create this matrix is omitted here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2bfea",
   "metadata": {},
   "source": [
    "## III. Encoder–Decoder for Neural Machine Translation\n",
    "\n",
    "Translate English to French using a Seq2Seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7446182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Placeholder variables for vocab sizes\n",
    "num_eng_tokens = 10000  # Adjust as per your data\n",
    "num_french_tokens = 10000  # Adjust as per your data\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
    "encoder_embedding = Embedding(num_eng_tokens, 256, name='encoder_embedding')(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
    "decoder_embedding = Embedding(num_french_tokens, 256, name='decoder_embedding')(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_french_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "seq2seq_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# To train, prepare your data accordingly.\n",
    "# seq2seq_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d3c17a",
   "metadata": {},
   "source": [
    "### A. Bidirectional RNNs for Contextual Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b69755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "bidirectional_layer = Bidirectional(LSTM(256))\n",
    "# Use in your model as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dff60c",
   "metadata": {},
   "source": [
    "### B. Beam Search for Improved Decoding\n",
    "\n",
    "- Implementing beam search improves the quality of sequence generation by considering multiple candidate sequences at each step.\n",
    "- Frameworks like TensorFlow Addons or custom implementations can be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88637081",
   "metadata": {},
   "source": [
    "## IV. Attention Mechanisms\n",
    "\n",
    "### A. Visual Attention (Bahdanau / Luong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98854494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Computing attention weights\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def bahdanau_attention(hidden_states, encoder_outputs):\n",
    "    # hidden_states: decoder hidden state\n",
    "    # encoder_outputs: all encoder outputs\n",
    "    # Implementation details omitted for brevity\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0905570",
   "metadata": {},
   "source": [
    "### B. Transformer: Attention Is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa93b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "attention_layer = MultiHeadAttention(num_heads=8, key_dim=64)\n",
    "# Example usage:\n",
    "query = ...  # shape: (batch_size, seq_len_q, depth)\n",
    "key = ...    # shape: (batch_size, seq_len_k, depth)\n",
    "value = ...  # shape: (batch_size, seq_len_v, depth)\n",
    "output = attention_layer(query=query, key=key, value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce66602",
   "metadata": {},
   "source": [
    "## V. Recent Innovations in NLP\n",
    "\n",
    "- Large models like GPT, BERT, and Transformer-based architectures.\n",
    "- Pretraining tasks such as masked language modeling and next-sentence prediction.\n",
    "- Fine-tuning on downstream tasks for state-of-the-art performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a456d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Character RNNs can generate stylistic text.\n",
    "- Sequence models with masking and pretrained embeddings improve classification.\n",
    "- Attention mechanisms enhance translation and understanding.\n",
    "- Transformers have revolutionized NLP.\n",
    "- Pretrained models like GPT and BERT are now standard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675b600",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Train a character RNN with different temperature settings to generate diverse styles.\n",
    "2. Compare performance of LSTM vs GRU on sentiment analysis.\n",
    "3. Implement Bahdanau attention in a custom Seq2Seq model.\n",
    "4. Build a small Transformer for English-French translation.\n",
    "5. Fine-tune a pretrained HuggingFace model for sentiment classification.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
