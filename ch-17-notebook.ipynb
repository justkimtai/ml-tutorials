{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d887be",
   "metadata": {},
   "source": [
    "# üé® Chapter 17: Autoencoders & GANs ‚Äî Practical Guide\n",
    "\n",
    "Explore how unsupervised deep learning can learn compact representations and generate realistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c113d51",
   "metadata": {},
   "source": [
    "## I. ü§è Efficient Data Representations\n",
    "\n",
    "Autoencoders learn compressed, meaningful representations by training to reconstruct their inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e716323",
   "metadata": {},
   "source": [
    "## II. üßä PCA with a Linear Autoencoder\n",
    "\n",
    "Let's start with a simple linear autoencoder that learns principal components (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Generate random data\n",
    "X = np.random.rand(1000, 20)\n",
    "\n",
    "# Define a simple linear autoencoder\n",
    "auto = models.Sequential([\n",
    "    layers.Dense(10, activation='linear', input_shape=(20,)),\n",
    "    layers.Dense(20, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "auto.compile(optimizer='adam', loss='mse')\n",
    "auto.fit(X, X, epochs=20, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9654ba",
   "metadata": {},
   "source": [
    "## III. üìö Stacked Autoencoders\n",
    "\n",
    "### A. Keras Implementation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train, _), (X_test, _) = fashion_mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "X_train = X_train.reshape((-1, 784))\n",
    "X_test = X_test.reshape((-1, 784))\n",
    "\n",
    "# Define encoder and decoder layers\n",
    "input_img = layers.Input(shape=(784,))\n",
    "hidden1 = layers.Dense(128, activation='relu')(input_img)\n",
    "hidden2 = layers.Dense(64, activation='relu')(hidden1)\n",
    "hidden3 = layers.Dense(128, activation='relu')(hidden2)\n",
    "output_layer = layers.Dense(784, activation='sigmoid')(hidden3)\n",
    "\n",
    "# Build autoencoder model\n",
    "stacked_ae = models.Model(inputs=input_img, outputs=output_layer)\n",
    "\n",
    "# Compile\n",
    "stacked_ae.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train\n",
    "stacked_ae.fit(X_train, X_train, epochs=10, batch_size=256, validation_data=(X_test, X_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7347383",
   "metadata": {},
   "source": [
    "### B. Visualize Reconstructions\n",
    "\n",
    "Let's see how well the autoencoder reconstructs some test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict reconstructions for first 10 test images\n",
    "recon = stacked_ae.predict(X_test[:10])\n",
    "\n",
    "# Plot original and reconstructed images\n",
    "plt.figure(figsize=(20,4))\n",
    "for i in range(10):\n",
    "    # Original\n",
    "    ax = plt.subplot(2, 10, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    # Reconstruction\n",
    "    ax = plt.subplot(2,10,i+11)\n",
    "    plt.imshow(recon[i].reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Top: Original | Bottom: Reconstruction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9179a8",
   "metadata": {},
   "source": [
    "## IV. üåÄ Convolutional Autoencoders\n",
    "\n",
    "Autoencoders can also be built with convolutional layers for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11def1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = layers.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "encoded = layers.MaxPooling2D(2, padding='same')(x)\n",
    "\n",
    "x = layers.Conv2D(32, 3, activation='relu', padding='same')(encoded)\n",
    "x = layers.UpSampling2D(2)(x)\n",
    "x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D(2)(x)\n",
    "decoded = layers.Conv2D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "conv_ae = models.Model(inputs=input_img, outputs=decoded)\n",
    "\n",
    "# Compile\n",
    "conv_ae.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Prepare data\n",
    "X_train_c = X_train.reshape(-1,28,28,1)\n",
    "\n",
    "# Train\n",
    "conv_ae.fit(X_train_c, X_train_c, epochs=10, batch_size=256, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aaa16a",
   "metadata": {},
   "source": [
    "## V. üîÅ Recurrent Autoencoders\n",
    "\n",
    "Recurrent autoencoders are suitable for sequence data like text or time series. Due to brevity, code is omitted, but they follow similar structure using RNNs or LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf707bd",
   "metadata": {},
   "source": [
    "## VI. üíß Denoising Autoencoders\n",
    "\n",
    "Train autoencoders to remove noise from inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0449c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add synthetic noise to training data\n",
    "noisy_X_train = X_train + np.random.normal(0, 0.5, X_train.shape)\n",
    "noisy_X_train = np.clip(noisy_X_train, 0., 1.)  # Keep within valid range\n",
    "\n",
    "# Retrain autoencoder on noisy data\n",
    "auto.compile(optimizer='adam', loss='mse')  # Recompile if needed\n",
    "auto.fit(noisy_X_train, X_train, epochs=10, batch_size=256, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ebc8e",
   "metadata": {},
   "source": [
    "## VII. ‚ùó Sparse Autoencoders\n",
    "\n",
    "Encourage sparsity in hidden units with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_encoder = layers.Dense(64, activation='relu', activity_regularizer=tf.keras.regularizers.l1(1e-5))\n",
    "\n",
    "# Build sparse autoencoder\n",
    "input_img = layers.Input(shape=(784,))\n",
    "hidden = sparse_encoder(input_img)\n",
    "output_layer = layers.Dense(784, activation='sigmoid')(hidden)\n",
    "sparse_autoencoder = models.Model(inputs=input_img, outputs=output_layer)\n",
    "\n",
    "# Compile\n",
    "sparse_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train\n",
    "sparse_autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d656fab7",
   "metadata": {},
   "source": [
    "## VIII. üåÄ Variational Autoencoders (VAEs)\n",
    "\n",
    "VAEs learn probabilistic latent representations, enabling realistic sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7ce2d",
   "metadata": {},
   "source": [
    "```python\n",
    "# High-level pseudocode for VAE implementation\n",
    "# 1. Encoder learns mean and log-variance of latent distribution\n",
    "# 2. Sample z via reparameterization trick\n",
    "# 3. Decoder reconstructs input from z\n",
    "# 4. Loss includes reconstruction + KL divergence\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8834c",
   "metadata": {},
   "source": [
    "## IX. ‚öîÔ∏è GANs (Generative Adversarial Networks)\n",
    "\n",
    "GANs involve training a generator and discriminator in a minimax game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33242b2d",
   "metadata": {},
   "source": [
    "### A. Deep Convolutional GAN Example\n",
    "\n",
    "Below is pseudocode for the typical training loop of a DCGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86183902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for training a DCGAN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define generator and discriminator models here...\n",
    "# For brevity, models are not fully implemented.\n",
    "\n",
    "# Training loop pseudocode:\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataset:\n",
    "        # 1. Train discriminator on real images\n",
    "        # 2. Generate fake images with generator\n",
    "        # 3. Train discriminator on fake images\n",
    "        # 4. Train generator to fool discriminator\n",
    "        pass\n",
    "\n",
    "# Note: Implementing full DCGAN training is more involved.\n",
    "# This pseudocode is a conceptual outline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e50d21",
   "metadata": {},
   "source": [
    "### B. Training Difficulties\n",
    "\n",
    "* ‚ö†Ô∏è Mode collapse, unstable convergence\n",
    "* Require careful balancing (learning rates, label smoothing, batch norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b909d",
   "metadata": {},
   "source": [
    "### C. Advanced Architectures\n",
    "\n",
    "* **DCGAN**: Stable convolutional GANs\n",
    "* **Progressive GANs**: Incrementally add layers during training\n",
    "* **StyleGAN**: Style-mixing via adaptive normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fbbcb8",
   "metadata": {},
   "source": [
    "## ‚úÖ Chapter Summary\n",
    "\n",
    "* **Autoencoders** learn compact, meaningful data representations.\n",
    "* **Variational autoencoders** model data generatively via latent distributions.\n",
    "* **GANs** generate realistic images by pitting two networks against each other.\n",
    "* Advanced GANs like **StyleGAN** produce high-resolution, photorealistic images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161d36c",
   "metadata": {},
   "source": [
    "## üß† Exercises to Try\n",
    "\n",
    "1. Implement a stacked autoencoder and measure compression vs reconstruction loss.\n",
    "2. Build and train a convolutional denoising autoencoder with added synthetic noise.\n",
    "3. Code a simple VAE and visualize generated Fashion MNIST samples.\n",
    "4. Build a DCGAN and train it‚Äîtrack real vs fake image quality over epochs.\n",
    "5. Try **Progressive Growing GANs** or **StyleGAN**, starting from a simpler version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
