{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Chapter 19: Training & Deploying TensorFlow Models at Scale\n",
    "\n",
    "This notebook provides hands-on, practical examples for serving, deploying, and scaling TensorFlow models. Follow along and adapt the code for your projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Serving a TensorFlow Model\n",
    "\n",
    "### A. Using **TensorFlow Serving**\n",
    "\n",
    "First, save your trained model in the SavedModel format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in SavedModel format\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example: create a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Save the model\n",
    "model.save(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run TensorFlow Serving using Docker:\n",
    "\n",
    "```bash\n",
    "# Launch TensorFlow Serving container\n",
    "docker run -p 8501:8501 \\\n",
    "  --mount type=bind,source=$(pwd)/my_model,target=/models/my_model \\\n",
    "  -e MODEL_NAME=my_model -t tensorflow/serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the server is running, send a prediction request via `curl`. Replace `[...]` with your input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -d '{\"instances\": [[1.0, 2.0, 3.0, ..., 20.0]]}' \\\n",
    "     -X POST http://localhost:8501/v1/models/my_model:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Using **GCP AI Platform Prediction** (Optional)\n",
    "\n",
    "You can deploy models to Google Cloud AI Platform for scalable serving. Here's a brief outline:\n",
    "\n",
    "```bash\n",
    "# Create model on GCP\n",
    "gcloud ai-platform models create my_model\n",
    "\n",
    "# Create a version with your model files in Google Cloud Storage\n",
    "gcloud ai-platform versions create v1 \\\n",
    "  --model=my_model \\\n",
    "  --origin=gs://my_bucket/my_model/ \\\n",
    "  --runtime-version=2.8 \\\n",
    "  --python-version=3.7\n",
    "```\n",
    "\n",
    "Then, send prediction requests using REST API or `gcloud` commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Deploying to Mobile & Embedded Devices\n",
    "\n",
    "Use **TensorFlow Lite** to run models on mobile or embedded hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SavedModel to TFLite\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your saved model\n",
    "saved_model_dir = \"my_model\"\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "print(\"TFLite model saved as 'model.tflite'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now load and run inference with the TFLite model in your mobile or embedded app using the TFLite Interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Using GPUs to Speed Up Computations\n",
    "\n",
    "### A. Ensuring GPU Availability\n",
    "\n",
    "Check if TensorFlow detects GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU Available: {len(gpus)}\")\n",
    "else:\n",
    "    print(\"No GPU detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Managing GPU Memory Growth\n",
    "\n",
    "Prevent TensorFlow from allocating all GPU memory upfront:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    print(\"Memory growth set for GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Placing Ops on GPU\n",
    "\n",
    "Verify GPU utilization by placing operations explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.random.uniform((1000, 1000))\n",
    "        b = tf.matmul(a, a)\n",
    "        print(f\"Running on device: {b.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Automatic Device Placement & Parallelism\n",
    "\n",
    "TensorFlow automatically distributes operations across available GPUs when using strategies like `tf.distribute.MirroredStrategy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Training Models Across Multiple Devices\n",
    "\n",
    "### A. Model vs Data Parallelism\n",
    "\n",
    "- **Model Parallelism**: split a complex model across devices.\n",
    "- **Data Parallelism**: replicate the model across devices and distribute data to train faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Using `tf.distribute.Strategy` for Data Parallelism\n",
    "\n",
    "Here's an example using `MirroredStrategy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_model()\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Prepare dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.rand(1000, 20)\n",
    "y_train = np.random.rand(1000, 1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Distributed Training on Cloud (GCP AI Platform)\n",
    "\n",
    "For large-scale training, configure a multi-worker setup and submit jobs via GCP AI Platform. Use `tf.distribute.MultiWorkerMirroredStrategy()` within your training script for multi-node training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. üõ†Ô∏è Exercises\n",
    "\n",
    "1. Containerize and deploy a TensorFlow model using Docker + TF Serving.\n",
    "2. Convert a CNN model to TFLite and run inference in Python.\n",
    "3. Write a TensorFlow script that places ops on GPU and verifies GPU utilization.\n",
    "4. Adapt an existing `model.fit()` script to use `tf.distribute.MirroredStrategy()`.\n",
    "5. Submit and monitor a distributed training job on GCP AI Platform, visualizing logs in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. üôè Thank You!\n",
    "\n",
    "This notebook equips you with the core tools to serve, scale, and maintain deep learning models‚Äîfrom edge devices to cloud-scale deployments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}