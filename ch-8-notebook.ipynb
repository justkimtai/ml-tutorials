{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17814a3b",
   "metadata": {},
   "source": [
    "# ðŸ“‰ Chapter 8: Dimensionality Reduction â€” Hands-On Guide\n",
    "\n",
    "Reducing the number of features (dimensions) helps with computation, visualization, and avoiding the \"Curse of Dimensionality.\" \n",
    "Let's explore key techniques with practical examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f9f26",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ðŸ§  The Curse of Dimensionality\n",
    "\n",
    "- In high dimensions:\n",
    "  - Data becomes sparse\n",
    "  - Distance metrics lose meaning\n",
    "  - Models tend to overfit\n",
    "\n",
    "**Solution:** apply dimensionality reduction to simplify data while retaining important structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7965650",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ðŸ” Main Approaches to Dimensionality Reduction\n",
    "\n",
    "- **Projection methods:** Find new axes (e.g., PCA)\n",
    "- **Manifold learning:** Preserve local relationships (e.g., Kernel PCA, LLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f2a22",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ðŸ§® Principal Component Analysis (PCA)\n",
    "\n",
    "- PCA finds orthogonal axes (principal components) that maximize data variance.\n",
    "- Useful for visualization, compression, and noise reduction.\n",
    "\n",
    "### A. Basic PCA projection to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba84681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "# Plot the projection\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(X2D[:, 0], X2D[:, 1], c=y, cmap='tab10', s=15)\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.title(\"Digits dataset projected via PCA\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051cd86",
   "metadata": {},
   "source": [
    "---\n",
    "### B. Variance Explained & Choosing Number of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72181dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with all components to examine explained variance\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X)\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "\n",
    "import numpy as np\n",
    "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = (cumulative_variance >= 0.95).argmax() + 1\n",
    "print(f\"Number of components for 95% variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5d1a3",
   "metadata": {},
   "source": [
    "---\n",
    "### C. PCA for Dimensionality Reduction (compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to enough components to preserve 95% variance\n",
    "pca_95 = PCA(n_components=0.95)\n",
    "X_reduced = pca_95.fit_transform(X)\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Reduced shape:\", X_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8505d",
   "metadata": {},
   "source": [
    "---\n",
    "### D. Fast PCA with Randomized SVD (for large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_rand = PCA(n_components=50, svd_solver='randomized', random_state=42)\n",
    "X_rand = pca_rand.fit_transform(X)\n",
    "print(\"Randomized PCA shape:\", X_rand.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdcf534",
   "metadata": {},
   "source": [
    "---\n",
    "### E. Incremental PCA (for large datasets or streaming data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ceee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# Simulate batching on the digits data\n",
    "batch_size = 100\n",
    "ipca = IncrementalPCA(n_components=50)\n",
    "for X_batch in np.array_split(X, len(X) // batch_size):\n",
    "    ipca.partial_fit(X_batch)\n",
    "\n",
    "X_ipca = ipca.transform(X)\n",
    "print(\"Incremental PCA shape:\", X_ipca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565047e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ðŸ§ª Nonlinear Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff10dda",
   "metadata": {},
   "source": [
    "- **Kernel PCA:** captures nonlinear structures via kernels.\n",
    "- **Locally Linear Embedding (LLE):** preserves local relationships.\n",
    "- **t-SNE:** excellent for visualization of high-dimensional data.\n",
    "- **Isomap:** preserves geodesic distances on manifolds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af6a729",
   "metadata": {},
   "source": [
    "---\n",
    "### A. Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c957527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Apply Kernel PCA with RBF kernel\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y, cmap='tab10', s=15)\n",
    "plt.title(\"Digits via Kernel PCA (RBF kernel)\")\n",
    "plt.xlabel(\"Kernel PC 1\")\n",
    "plt.ylabel(\"Kernel PC 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c646ec",
   "metadata": {},
   "source": [
    "---\n",
    "### B. Locally Linear Embedding (LLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01838dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=30, method='standard')\n",
    "X_lle = lle.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_lle[:, 0], X_lle[:, 1], c=y, cmap='tab10', s=15)\n",
    "plt.title(\"Digits via LLE\")\n",
    "plt.xlabel(\"LLE Dim 1\")\n",
    "plt.ylabel(\"LLE Dim 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f41c7",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Summary & Use Cases\n",
    "\n",
    "| Technique | Type | Use Case |\n",
    "| --- | --- | --- |\n",
    "| PCA | Projection | Linear reduction, compression |\n",
    "| Randomized PCA | Projection | Large datasets |\n",
    "| Incremental PCA | Projection | Streaming data |\n",
    "| Kernel PCA | Nonlinear | Nonlinear structures |\n",
    "| LLE | Manifold learning | Preserve local structure |\n",
    "| t-SNE | Visualization | High-dimensional clustering |\n",
    "| Isomap | Manifold | Geodesic distances |\n",
    "\n",
    "Choose based on dataset size, linearity, and visualization needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f81e7b",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Practice Exercises\n",
    "\n",
    "1. Apply PCA on a 5D dataset, reconstruct original data, and compute reconstruction error.\n",
    "2. Compare Kernel PCA with RBF vs polynomial kernels on labeled data.\n",
    "3. Use LLE and Isomap on a Swiss roll dataset.\n",
    "4. Visualize MNIST digits with t-SNE to identify clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
