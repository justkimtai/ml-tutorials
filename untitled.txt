{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü Chapter 7: Ensemble Learning & Random Forests ‚Äî Practical Guide\n",
    "\n",
    "Ensemble learning combines multiple models to make more accurate and robust predictions.\n",
    "\n",
    "We'll explore different ensemble techniques with Python examples using scikit-learn.\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üîπ Voting Classifiers\n",
    "\n",
    "Voting classifiers combine predictions from different models and make a final decision based on majority or weighted voting.\n",
    "\n",
    "We'll use the Iris dataset and combine Logistic Regression, Decision Tree, and SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, random_state=42)\n",
    "\n",
    "# Define individual classifiers\n",
    "log_clf = LogisticRegression(max_iter=1000)\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "svm_clf = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Create a VotingClassifier with soft voting\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('dt', tree_clf), ('svm', svm_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Fit and evaluate\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Voting Classifier Accuracy:\", accuracy_score(y_test, y_pred))"
  ],
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üîÅ Bagging and Pasting\n",
    "\n",
    "Bagging (Bootstrap Aggregating) trains multiple models on different random subsets of data.\n",
    "Pasting is similar but without replacement.\n",
    "\n",
    "We'll use Decision Trees as base estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Bagging with bootstrap samples\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    max_samples=0.8,\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, bag_clf.predict(X_test)))"
  ],
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag (OOB) Samples for Validation\n",
    "\n",
    "OOB samples are the data points not used in training each bootstrap sample. They can be used for validation.\n",
    "\n",
    "Let's see how to access the OOB score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging with OOB score enabled\n",
    "bag_clf_oob = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "bag_clf_oob.fit(X_train, y_train)\n",
    "print(\"OOB Score:\", bag_clf_oob.oob_score_)"
  ],
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üå≤ Random Patches and Random Subspaces ‚Äî Random Forests & Extra Trees\n",
    "\n",
    "These methods introduce randomness across features and samples to create diverse trees.\n",
    "Random Forests are the most popular implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "# Initialize classifiers\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train classifiers\n",
    "rf_clf.fit(X_train, y_train)\n",
    "et_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_clf.predict(X_test)))\n",
    "print(\"Extra Trees Accuracy:\", accuracy_score(y_test, et_clf.predict(X_test)))"
  ],
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Feature Importance\n",
    "\n",
    "These models can also tell us which features are most important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in [(\"Random Forest\", rf_clf), (\"Extra Trees\", et_clf)]:\n",
    "    print(name, \"feature importances:\", clf.feature_importances_)"
  ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üöÄ Boosting\n",
    "\n",
    "Boosting trains models sequentially, where each new model tries to correct errors made by previous ones.\n",
    "\n",
    "We'll explore AdaBoost and Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# AdaBoost with decision stumps\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, ada_clf.predict(X_test)))"
  ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Gradient Boosting\n",
    "\n",
    "Gradient Boosting builds models sequentially by optimizing a loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=1.0,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "gb_clf.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, gb_clf.predict(X_test)))"
  ],
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üéØ Stacking\n",
    "\n",
    "Stacking combines diverse models and trains a meta-classifier on their outputs.\n",
    "It's a way to leverage the strengths of different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize stacking classifier with base models and meta-model\n",
    "stack_clf = StackingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('et', et_clf), ('gb', gb_clf)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "stack_clf.fit(X_train, y_train)\n",
    "print(\"Stacking Accuracy:\", accuracy_score(y_test, stack_clf.predict(X_test)))"
  ],
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table\n",
    "\n",
    "| Technique                        | Description                                                            |\n",
    "| -------------------------------- | ---------------------------------------------------------------------- |\n",
    "| **Voting**                       | Combines predictions from diverse models                               |\n",
    "| **Bagging / Pasting**            | Trains multiple bootstrapped/pasted models                             |\n",
    "| **Random Forest / Extra Trees**  | Adds feature randomness for diverse trees; computes feature importance |\n",
    "| **AdaBoost / Gradient Boosting** | Sequentially corrects previous errors                                  |\n",
    "| **Stacking**                     | A meta-model learns from base model outputs                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Exercises to Practice\n",
    "\n",
    "1. Tune `n_estimators`, `max_depth`, and `max_features` for Random Forest using `GridSearchCV`.\n",
    "2. Plot learning curves for AdaBoost and Gradient Boosting to check for overfitting or underfitting.\n",
    "3. Compare stacking versus a voting classifier on a larger or more complex dataset.\n",
    "4. Analyze misclassifications from the stacked model to understand weaknesses.\n",
    "\n",
    "Happy experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}